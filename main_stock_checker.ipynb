{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SETUP THE VARIABLES AND DO THE IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dao\n",
    "import pandas as pd\n",
    "import prediction\n",
    "import backtesting\n",
    "import prediction\n",
    "import config\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import utils\n",
    "import simulation\n",
    "import concurrent.futures\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "current_date_time = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the run settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock equity research filetype pdf\n",
    "run_type = \"BOUGHT\"  # options --> GENERIC, BOUGHT, SIM\n",
    "initial_funds = 1000  # funds per stock for simulation\n",
    "# current_date = \"20241124\" #comment out for latest date run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "declare the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_exec_flag = False\n",
    "exploratory_exec_flag = False\n",
    "\n",
    "data_dump_path = config.path_data_dump + run_type + \"\\\\\"\n",
    "static_files_path = config.path_static_folder\n",
    "summaries_files_path = config.path_summaries_folder + run_type + \"\\\\\"\n",
    "stocks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the stock list based on run type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAPL', 'NVDA', 'TSLA', 'AMZN', 'AMD', 'NFLX', 'MSFT', 'GOOGL', 'DIS', 'META', 'SMR', 'WGS', 'MSTR', 'SMTC', 'CSAN', 'SBS', 'DB', 'FIVE', 'TMC', 'RDDT', 'SYM']\n"
     ]
    }
   ],
   "source": [
    "if run_type == \"GENERIC\":\n",
    "    count = 50\n",
    "    stock_filters = [config.WeekGainers52, config.WeekLosers52] #MostActive, Gainers, Losers, TrendingNow, WeekGainers52, WeekLosers52\n",
    "    for stock_filter in stock_filters:\n",
    "        stocks += dao.get_yahoo_finance_stock_list(\n",
    "            stock_filter+config.stock_list_appender, count)\n",
    "    stocks = pd.unique(stocks).tolist()\n",
    "    exploratory_exec_flag = True\n",
    "    \n",
    "elif run_type == \"BOUGHT\":\n",
    "    portfolio_json_file = \"real_portfolio.json\"\n",
    "    portfolio_file = static_files_path + portfolio_json_file\n",
    "    portfolio_stocks = dao.read_json(portfolio_file)\n",
    "    for item in portfolio_stocks:\n",
    "        stocks.append(item['ticker'])\n",
    "    portfolio_exec_flag = True\n",
    "    \n",
    "elif run_type == \"SIM\":\n",
    "    portfolio_json_file = \"sim_portfolio.json\"\n",
    "    portfolio_file = static_files_path + portfolio_json_file\n",
    "    portfolio_stocks = dao.read_json(portfolio_file)\n",
    "    for item in portfolio_stocks:\n",
    "        stocks.append(item['ticker'])\n",
    "    portfolio_exec_flag = True\n",
    "\n",
    "print(stocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data for each stock and create predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error in get_yahoo_finance_5y for AAPL\n",
      "ERROR:root:Error in get_yahoo_finance_5y for NVDA\n",
      "ERROR:root:Error in get_yahoo_finance_5y for TSLA\n",
      "ERROR:root:Error in get_yahoo_finance_5y for AMZN\n",
      "ERROR:root:Error in get_yahoo_finance_5y for AMD\n",
      "ERROR:root:Error in get_yahoo_finance_5y for NFLX\n",
      "ERROR:root:Error in get_yahoo_finance_5y for MSFT\n",
      "ERROR:root:Error in get_yahoo_finance_5y for GOOGL\n",
      "ERROR:root:Error in get_yahoo_finance_5y for DIS\n",
      "ERROR:root:Error in get_yahoo_finance_5y for META\n",
      "ERROR:root:Error in get_yahoo_finance_5y for SMR\n",
      "ERROR:root:Error in get_yahoo_finance_5y for WGS\n",
      "ERROR:root:Error in get_yahoo_finance_5y for MSTR\n",
      "ERROR:root:Error in get_yahoo_finance_5y for SMTC\n",
      "ERROR:root:Error in get_yahoo_finance_5y for CSAN\n",
      "ERROR:root:Error in get_yahoo_finance_5y for SBS\n",
      "ERROR:root:Error in get_yahoo_finance_5y for DB\n",
      "ERROR:root:Error in get_yahoo_finance_5y for FIVE\n",
      "ERROR:root:Error in get_yahoo_finance_5y for TMC\n",
      "ERROR:root:Error in get_yahoo_finance_5y for RDDT\n",
      "ERROR:root:Error in get_yahoo_finance_5y for SYM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL processing for 20250726\n",
      "NVDA processing for 20250726\n",
      "TSLA processing for 20250726\n",
      "AMZN processing for 20250726\n",
      "AMD processing for 20250726\n",
      "NFLX processing for 20250726\n",
      "MSFT processing for 20250726\n",
      "GOOGL processing for 20250726\n",
      "DIS processing for 20250726\n",
      "META processing for 20250726\n",
      "SMR processing for 20250726\n",
      "WGS processing for 20250726\n",
      "MSTR processing for 20250726\n",
      "SMTC processing for 20250726\n",
      "CSAN processing for 20250726\n",
      "SBS processing for 20250726\n",
      "DB processing for 20250726\n",
      "FIVE processing for 20250726\n",
      "TMC processing for 20250726\n",
      "RDDT processing for 20250726\n",
      "SYM processing for 20250726\n"
     ]
    }
   ],
   "source": [
    "# Function to process each stock\n",
    "def process_stock(stock, data_dump_path, run_type, current_date):\n",
    "    print(f\"{stock} processing for {current_date}\")\n",
    "    # Declare the files\n",
    "    stats_file = data_dump_path + run_type + \"_\" + \\\n",
    "        stock + \"_df_stats_\" + str(current_date) + \".csv\"\n",
    "    pred_file = data_dump_path + run_type + \"_\" + \\\n",
    "        stock + \"_df_pred_\" + str(current_date) + \".csv\"\n",
    "\n",
    "    if not os.path.exists(pred_file):\n",
    "        # Get the required data from Yahoo Finance\n",
    "        df_5y = pd.DataFrame(data=dao.get_yahoo_finance_5y(stock))\n",
    "        df_stats = pd.DataFrame(\n",
    "            data=dao.get_yahoo_finance_key_stats(stock)).iloc[0]\n",
    "\n",
    "        # Write the stats data\n",
    "        dao.write_csv(stats_file, df_stats)\n",
    "\n",
    "        # Get the prediction\n",
    "        df_pred = prediction.get_prediction(df_5y, df_stats)\n",
    "\n",
    "        # Calculate the signal (Signal calculation logic)\n",
    "        df_pred['Signal'] = df_pred['technical_analysis_buy_score'] + df_pred['technical_analysis_sell_score'] + \\\n",
    "            df_pred['fundamental_analysis_score'] + \\\n",
    "            df_pred['sentiment_analysis_score']\n",
    "\n",
    "        # Convert the signal to text\n",
    "        df_pred = prediction.convert_signal_to_text(df_pred)\n",
    "\n",
    "        # Write the prediction data\n",
    "        dao.write_csv(pred_file, df_pred)\n",
    "\n",
    "    print(f\"{stock} processed for {current_date}\")\n",
    "\n",
    "# Main function to execute multi-threading\n",
    "def process_stocks_in_parallel(stocks, data_dump_path, run_type, current_date):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        executor.map(lambda stock: process_stock(\n",
    "            stock, data_dump_path, run_type, current_date), stocks)\n",
    "\n",
    "\n",
    "# Call the main function to process the stocks in parallel\n",
    "process_stocks_in_parallel(stocks, data_dump_path, run_type, current_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backtest the predictions and create the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL\n",
      "NVDA\n",
      "TSLA\n",
      "AMZN\n",
      "AMD\n",
      "NFLX\n",
      "MSFT\n",
      "GOOGL\n",
      "DIS\n",
      "META\n",
      "SMR\n",
      "WGS\n",
      "MSTR\n",
      "SMTC\n",
      "CSAN\n",
      "SBS\n",
      "DB\n",
      "FIVE\n",
      "RDDT\n",
      "SYM\n"
     ]
    }
   ],
   "source": [
    "# GET THE SUMMARY FILES IF IT EXISTS ELSE CREATE NEW\n",
    "prediction_summary_file = summaries_files_path + run_type + \\\n",
    "    \"_prediction_summary_\"+str(current_date)+\".csv\"\n",
    "prediction_summary_df = utils.get_summary_file(name=prediction_summary_file)\n",
    "\n",
    "# LOOP OVER STOCKS AND PROCESS EACH STOCK\n",
    "for stock in stocks:\n",
    "    print(stock)\n",
    "\n",
    "    # DECLARE THE FILES\n",
    "    pred_file = data_dump_path + run_type + \"_\" + \\\n",
    "        stock + \"_df_pred_\" + str(current_date) + \".csv\"\n",
    "    tested_file = data_dump_path + run_type + \"_\" + \\\n",
    "        stock + \"_df_tested_\" + str(current_date) + \".csv\"\n",
    "\n",
    "    # READ LOAD THE PRED FILE\n",
    "    file_to_test_df = pd.read_csv(pred_file)\n",
    "\n",
    "    # BACKTEST THE SIGNAL\n",
    "    df_tested, cumulative_return, sharpe_ratio, sortino_ratio, max_drawdown, calmar_ratio = backtesting.backtest(\n",
    "        file_to_test_df)\n",
    "\n",
    "    # WRITE THE PREDICTION BACKTESTED DATA\n",
    "    dao.write_csv(tested_file, df_tested)\n",
    "\n",
    "    # GET THE SUMMARY\n",
    "    start_iloc = 1\n",
    "    end_iloc = 30\n",
    "    signals = df_tested[\"Signal_Text\"].iloc[start_iloc:end_iloc +\n",
    "                                            1].reset_index(drop=True)\n",
    "    signal_text, signal_number = prediction.get_weighted_signal(signals)\n",
    "    prediction_summary_df = pd.concat([prediction_summary_df,\n",
    "                                       pd.DataFrame([[\n",
    "                                           current_date,\n",
    "                                           stock,\n",
    "                                           signal_number,\n",
    "                                           signal_text,\n",
    "                                           cumulative_return,\n",
    "                                           sharpe_ratio,\n",
    "                                           sortino_ratio,\n",
    "                                           max_drawdown,\n",
    "                                           calmar_ratio\n",
    "                                       ]],\n",
    "                                           columns=prediction_summary_df.columns)],\n",
    "                                      ignore_index=True)\n",
    "    prediction_summary_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # WRITE THE SUMMARY DATA\n",
    "    dao.write_csv(prediction_summary_file, prediction_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate exploratory trades from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exploratory_exec_flag set to: False\n"
     ]
    }
   ],
   "source": [
    "if exploratory_exec_flag:\n",
    "    for stock in stocks:\n",
    "        print(stock)\n",
    "        # DECLARE THE FILES\n",
    "        pred_file = data_dump_path + run_type + \"_\" + \\\n",
    "            stock + \"_df_pred_\" + str(current_date) + \".csv\"\n",
    "        exploratory_simulation_file = data_dump_path + run_type + \"_\" + \\\n",
    "            stock + \"_exploratory_simulation_\" + str(current_date) + \".json\"\n",
    "\n",
    "        # PREPARE THE SIM DATA\n",
    "        file_to_sim_df = pd.read_csv(pred_file)\n",
    "        data_to_sim_df = file_to_sim_df[[\n",
    "            'High', 'Low', 'Close', 'Date', 'TICKER', 'Signal_Text']]\n",
    "\n",
    "        # INITIALISE THE SIM VALUES\n",
    "        start_iloc = 1  # 0 is the latest day\n",
    "        end_iloc = 31  # 29 is the 30th day\n",
    "\n",
    "        # DO THE SIM\n",
    "        simulation_df = simulation.simulate_exploratory_trading(\n",
    "            data_to_sim_df, start_iloc, end_iloc, initial_funds)\n",
    "\n",
    "        # WRITE THE DATA\n",
    "        dao.write_json(exploratory_simulation_file, simulation_df)\n",
    "else:\n",
    "    print('exploratory_exec_flag set to: '+str(exploratory_exec_flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarise the exploratory simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exploratory_exec_flag set to: False\n"
     ]
    }
   ],
   "source": [
    "if exploratory_exec_flag:\n",
    "    # GET THE SUMMARY FILES IF IT EXISTS ELSE CREATE NEW\n",
    "    exploratory_simulation_summary_file = summaries_files_path + run_type + \\\n",
    "        \"_exploratory_simulation_summary_\"+str(current_date)+\".csv\"\n",
    "    exploratory_simulation_summary_df = utils.get_simulation_file(\n",
    "        exploratory_simulation_summary_file)\n",
    "\n",
    "    # LOOP OVER STOCKS AND PROCESS EACH STOCK\n",
    "    for stock in stocks:\n",
    "        print(stock)\n",
    "\n",
    "        # DECLARE THE FILES\n",
    "        exploratory_simulation_file = data_dump_path + run_type + \"_\" + \\\n",
    "            stock + \"_exploratory_simulation_\" + str(current_date) + \".json\"\n",
    "\n",
    "        # READ FILE\n",
    "        file_to_json = dao.read_json(exploratory_simulation_file)\n",
    "\n",
    "        positions = []\n",
    "        for json in file_to_json['transactions']:\n",
    "            positions.append(json['position'])\n",
    "        series = pd.Series(positions)\n",
    "        days_to_consider = len(positions)\n",
    "\n",
    "        signal_text, signal_number = prediction.get_weighted_signal(series)\n",
    "\n",
    "        exploratory_simulation_summary_df = pd.concat([exploratory_simulation_summary_df,\n",
    "                                                       pd.DataFrame([[\n",
    "                                                           current_date,\n",
    "                                                           stock,\n",
    "                                                           file_to_json['closing_stock_price'],\n",
    "                                                           file_to_json['final_cash_balance'],\n",
    "                                                           file_to_json['unrealized_gains_losses'],\n",
    "                                                           file_to_json['unrealized_gain_loss_%'],\n",
    "                                                           file_to_json['units_held'],\n",
    "                                                           file_to_json['average_price_per_unit'],\n",
    "                                                           signal_text,\n",
    "                                                           signal_number\n",
    "                                                       ]],\n",
    "                                                           columns=exploratory_simulation_summary_df.columns)],\n",
    "                                                      ignore_index=True)\n",
    "        exploratory_simulation_summary_df.drop_duplicates(inplace=True)\n",
    "\n",
    "        dao.write_csv(exploratory_simulation_summary_file,\n",
    "                      exploratory_simulation_summary_df)\n",
    "else:\n",
    "    print('exploratory_exec_flag set to: '+str(exploratory_exec_flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate current portfolio trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ticker': 'AAPL', 'date': '2024-12-02', 'units': 0.044192, 'average_buying_price': 52.72}\n",
      "{'ticker': 'NVDA', 'date': '2024-12-04', 'units': 0.0415, 'average_buying_price': 138.55}\n",
      "{'ticker': 'TSLA', 'date': '2024-11-19', 'units': 0.002977, 'average_buying_price': 335.91}\n",
      "{'ticker': 'AMZN', 'date': '2024-11-19', 'units': 0.005041, 'average_buying_price': 198.37}\n",
      "{'ticker': 'AMD', 'date': '2024-12-04', 'units': 0.093372, 'average_buying_price': 137.51}\n",
      "{'ticker': 'NFLX', 'date': '2024-11-19', 'units': 0.00119, 'average_buying_price': 840.34}\n",
      "{'ticker': 'MSFT', 'date': '2024-11-19', 'units': 0.002416, 'average_buying_price': 413.91}\n",
      "{'ticker': 'GOOGL', 'date': '2024-11-19', 'units': 0.005736, 'average_buying_price': 174.34}\n",
      "{'ticker': 'DIS', 'date': '2024-11-19', 'units': 0.008897, 'average_buying_price': 112.4}\n",
      "{'ticker': 'META', 'date': '2024-11-19', 'units': 0.00502, 'average_buying_price': 597.61}\n",
      "{'ticker': 'SMR', 'date': '2024-11-19', 'units': 0.0741, 'average_buying_price': 26.99}\n",
      "{'ticker': 'WGS', 'date': '2024-12-02', 'units': 0.06855, 'average_buying_price': 78.05}\n",
      "{'ticker': 'MSTR', 'date': '2024-12-02', 'units': 0.006117, 'average_buying_price': 393.98}\n",
      "{'ticker': 'SMTC', 'date': '2024-12-02', 'units': 0.034824, 'average_buying_price': 64.61}\n",
      "{'ticker': 'CSAN', 'date': '2024-12-04', 'units': 0.761575, 'average_buying_price': 6.57}\n",
      "{'ticker': 'SBS', 'date': '2024-12-02', 'units': 0.216748, 'average_buying_price': 15.23}\n",
      "{'ticker': 'DB', 'date': '2024-12-02', 'units': 0.175695, 'average_buying_price': 17.08}\n",
      "{'ticker': 'FIVE', 'date': '2024-12-02', 'units': 0.02819, 'average_buying_price': 96.84}\n",
      "{'ticker': 'RDDT', 'date': '2024-12-04', 'units': 0.01923, 'average_buying_price': 169.01}\n",
      "{'ticker': 'SYM', 'date': '2024-12-04', 'units': 0.11208, 'average_buying_price': 28.55}\n"
     ]
    }
   ],
   "source": [
    "if portfolio_exec_flag:\n",
    "    portfolio_file = static_files_path + portfolio_json_file\n",
    "    portfolio_stocks = dao.read_json(portfolio_file)\n",
    "\n",
    "    for stock in portfolio_stocks:\n",
    "        print(stock)\n",
    "\n",
    "        # DECLARE THE FILES\n",
    "        pred_file = data_dump_path + run_type + \"_\" + \\\n",
    "            stock['ticker'] + \"_df_pred_\" + str(current_date) + \".csv\"\n",
    "        portfolio_simulation_file = data_dump_path + run_type + \"_\" + \\\n",
    "            stock['ticker'] + \"_portfolio_simulation_\" + \\\n",
    "            str(current_date) + \".json\"\n",
    "\n",
    "        # PREPARE THE SIM DATA\n",
    "        file_to_sim_df = pd.read_csv(pred_file)\n",
    "        data_to_sim_df = file_to_sim_df[[\n",
    "            'High', 'Low', 'Close', 'Date', 'TICKER', 'Signal_Text']]\n",
    "\n",
    "        # GET ILOCS\n",
    "        start_iloc = 1\n",
    "        end_iloc = data_to_sim_df.index[data_to_sim_df['Date'] == stock['date']].tolist()[\n",
    "            0]\n",
    "        units_held = stock['units']\n",
    "        avg_price = stock['average_buying_price']\n",
    "\n",
    "        # DO THE SIM\n",
    "        simulation_df = simulation.simulate_portfolio_trades(\n",
    "            data_to_sim_df, start_iloc, end_iloc, initial_funds, units_held, avg_price)\n",
    "\n",
    "        # WRITE THE DATA\n",
    "        dao.write_json(portfolio_simulation_file, simulation_df)\n",
    "else:\n",
    "    print('portfolio_exec_flag set to: '+str(portfolio_exec_flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarise the potrfolio simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL\n",
      "NVDA\n",
      "TSLA\n",
      "AMZN\n",
      "AMD\n",
      "NFLX\n",
      "MSFT\n",
      "GOOGL\n",
      "DIS\n",
      "META\n",
      "SMR\n",
      "WGS\n",
      "MSTR\n",
      "SMTC\n",
      "CSAN\n",
      "SBS\n",
      "DB\n",
      "FIVE\n",
      "RDDT\n",
      "SYM\n"
     ]
    }
   ],
   "source": [
    "if portfolio_exec_flag:\n",
    "    # GET THE SUMMARY FILES IF IT EXISTS ELSE CREATE NEW\n",
    "    portfolio_simulation_summary_file = summaries_files_path + run_type + \\\n",
    "        \"_portfolio_simulation_summary_\"+str(current_date)+\".csv\"\n",
    "    portfolio_simulation_summary_df = utils.get_simulation_file(\n",
    "        portfolio_simulation_summary_file)\n",
    "\n",
    "    # LOOP OVER STOCKS AND PROCESS EACH STOCK\n",
    "    for stock in stocks:\n",
    "        print(stock)\n",
    "\n",
    "        # DECLARE THE FILES\n",
    "        portfolio_simulation_file = data_dump_path + run_type + \"_\" + \\\n",
    "            stock + \"_portfolio_simulation_\" + str(current_date) + \".json\"\n",
    "\n",
    "        # READ FILE\n",
    "        file_to_json = dao.read_json(portfolio_simulation_file)\n",
    "\n",
    "        positions = []\n",
    "        for json in file_to_json['transactions']:\n",
    "            positions.append(json['position'])\n",
    "        series = pd.Series(positions)\n",
    "        days_to_consider = len(positions)\n",
    "\n",
    "        signal_text, signal_number = prediction.get_weighted_signal(series)\n",
    "\n",
    "        portfolio_simulation_summary_df = pd.concat([portfolio_simulation_summary_df,\n",
    "                                                     pd.DataFrame([[\n",
    "                                                         current_date,\n",
    "                                                         stock,\n",
    "                                                         file_to_json['closing_stock_price'],\n",
    "                                                         file_to_json['final_cash_balance'],\n",
    "                                                         file_to_json['unrealized_gains_losses'],\n",
    "                                                         file_to_json['unrealized_gain_loss_%'],\n",
    "                                                         file_to_json['units_held'],\n",
    "                                                         file_to_json['average_price_per_unit'],\n",
    "                                                         signal_text,\n",
    "                                                         signal_number\n",
    "                                                     ]],\n",
    "                                                         columns=portfolio_simulation_summary_df.columns)],\n",
    "                                                    ignore_index=True)\n",
    "        portfolio_simulation_summary_df.drop_duplicates(inplace=True)\n",
    "\n",
    "        dao.write_csv(portfolio_simulation_summary_file,\n",
    "                      portfolio_simulation_summary_df)\n",
    "else:\n",
    "    print('portfolio_exec_flag set to: '+str(portfolio_exec_flag))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
