{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SETUP THE VARIABLES AND DO THE IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dao\n",
    "import pandas as pd\n",
    "import prediction\n",
    "import backtesting\n",
    "import prediction\n",
    "import config\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import utils\n",
    "import simulation\n",
    "import concurrent.futures\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "current_date_time = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the run settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock equity research filetype pdf\n",
    "run_type = \"BOUGHT\"  # options --> GENERIC, BOUGHT, SIM\n",
    "initial_funds = 1000  # funds per stock for simulation\n",
    "# current_date = \"20241124\" #comment out for latest date run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "declare the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_exec_flag = False\n",
    "exploratory_exec_flag = False\n",
    "\n",
    "data_dump_path = config.path_data_dump + run_type + \"\\\\\"\n",
    "static_files_path = config.path_static_folder\n",
    "summaries_files_path = config.path_summaries_folder + run_type + \"\\\\\"\n",
    "stocks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the stock list based on run type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAPL', 'NVDA', 'TSLA', 'AMZN', 'AMD', 'NFLX', 'MSFT', 'GOOGL', 'DIS', 'META', 'SMR', 'SMMT', 'WGS', 'MSTR', 'SMTC', 'STLA', 'CSAN', 'SBS', 'DB', 'FIVE', 'NVCR', 'TMC']\n"
     ]
    }
   ],
   "source": [
    "if run_type == \"GENERIC\":\n",
    "    count = 50\n",
    "    stock_filters = [config.WeekGainers52, config.WeekLosers52] #MostActive, Gainers, Losers, TrendingNow, WeekGainers52, WeekLosers52\n",
    "    for stock_filter in stock_filters:\n",
    "        stocks += dao.get_yahoo_finance_stock_list(\n",
    "            stock_filter+config.stock_list_appender, count)\n",
    "    stocks = pd.unique(stocks).tolist()\n",
    "    exploratory_exec_flag = True\n",
    "    \n",
    "elif run_type == \"BOUGHT\":\n",
    "    portfolio_json_file = \"real_portfolio.json\"\n",
    "    portfolio_file = static_files_path + portfolio_json_file\n",
    "    portfolio_stocks = dao.read_json(portfolio_file)\n",
    "    for item in portfolio_stocks:\n",
    "        stocks.append(item['ticker'])\n",
    "    portfolio_exec_flag = True\n",
    "    \n",
    "elif run_type == \"SIM\":\n",
    "    portfolio_json_file = \"sim_portfolio.json\"\n",
    "    portfolio_file = static_files_path + portfolio_json_file\n",
    "    portfolio_stocks = dao.read_json(portfolio_file)\n",
    "    for item in portfolio_stocks:\n",
    "        stocks.append(item['ticker'])\n",
    "    portfolio_exec_flag = True\n",
    "\n",
    "print(stocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data for each stock and create predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL processing for 20250113\n",
      "NVDA processing for 20250113\n",
      "TSLA processing for 20250113\n",
      "AMZN processing for 20250113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error in get_reddit_links for TSLA\n",
      "ERROR:root:Error in get_reddit_links for AMZN\n",
      "ERROR:root:Error in get_reddit_links for NVDA\n",
      "ERROR:root:Error in get_reddit_links for AAPL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMZN processed for 20250113\n",
      "AMD processing for 20250113\n",
      "TSLA processed for 20250113\n",
      "NFLX processing for 20250113\n",
      "NVDA processed for 20250113\n",
      "MSFT processing for 20250113\n",
      "AAPL processed for 20250113\n",
      "GOOGL processing for 20250113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error in get_reddit_links for AMD\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMD processed for 20250113\n",
      "DIS processing for 20250113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error in get_reddit_links for MSFT\n",
      "ERROR:root:Error in get_reddit_links for NFLX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSFT processed for 20250113\n",
      "META processing for 20250113\n",
      "NFLX processed for 20250113\n",
      "SMR processing for 20250113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error in get_reddit_links for GOOGL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGL processed for 20250113\n",
      "SMMT processing for 20250113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error in get_reddit_links for DIS\n",
      "ERROR:root:Error in get_reddit_links for META\n",
      "ERROR:root:Error in get_reddit_links for SMR\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIS processed for 20250113\n",
      "WGS processing for 20250113\n",
      "META processed for 20250113\n",
      "MSTR processing for 20250113\n",
      "SMR processed for 20250113\n",
      "SMTC processing for 20250113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error in get_reddit_links for SMMT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMMT processed for 20250113\n",
      "STLA processing for 20250113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error in get_reddit_links for WGS\n",
      "ERROR:root:Error in get_reddit_links for MSTR\n",
      "ERROR:root:Error in get_reddit_links for SMTC\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WGS processed for 20250113\n",
      "CSAN processing for 20250113\n",
      "SMTC processed for 20250113\n",
      "SBS processing for 20250113\n",
      "MSTR processed for 20250113\n",
      "DB processing for 20250113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error in get_reddit_links for STLA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STLA processed for 20250113\n",
      "FIVE processing for 20250113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error in get_reddit_links for CSAN\n",
      "ERROR:root:Error in get_reddit_links for DB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSAN processed for 20250113\n",
      "NVCR processing for 20250113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error in get_reddit_links for SBS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB processed for 20250113\n",
      "TMC processing for 20250113\n",
      "SBS processed for 20250113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error in get_reddit_links for NVCR\n",
      "ERROR:root:Error in get_reddit_links for TMC\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVCR processed for 20250113\n",
      "TMC processed for 20250113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error in get_reddit_links for FIVE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIVE processed for 20250113\n"
     ]
    }
   ],
   "source": [
    "# Function to process each stock\n",
    "def process_stock(stock, data_dump_path, run_type, current_date):\n",
    "    print(f\"{stock} processing for {current_date}\")\n",
    "    # Declare the files\n",
    "    stats_file = data_dump_path + run_type + \"_\" + \\\n",
    "        stock + \"_df_stats_\" + str(current_date) + \".csv\"\n",
    "    pred_file = data_dump_path + run_type + \"_\" + \\\n",
    "        stock + \"_df_pred_\" + str(current_date) + \".csv\"\n",
    "\n",
    "    if not os.path.exists(pred_file):\n",
    "        # Get the required data from Yahoo Finance\n",
    "        df_5y = pd.DataFrame(data=dao.get_yahoo_finance_5y(stock))\n",
    "        df_stats = pd.DataFrame(\n",
    "            data=dao.get_yahoo_finance_key_stats(stock)).iloc[0]\n",
    "\n",
    "        # Write the stats data\n",
    "        dao.write_csv(stats_file, df_stats)\n",
    "\n",
    "        # Get the prediction\n",
    "        df_pred = prediction.get_prediction(df_5y, df_stats)\n",
    "\n",
    "        # Calculate the signal (Signal calculation logic)\n",
    "        df_pred['Signal'] = df_pred['technical_analysis_buy_score'] + df_pred['technical_analysis_sell_score'] + \\\n",
    "            df_pred['fundamental_analysis_score'] + \\\n",
    "            df_pred['sentiment_analysis_score']\n",
    "\n",
    "        # Convert the signal to text\n",
    "        df_pred = prediction.convert_signal_to_text(df_pred)\n",
    "\n",
    "        # Write the prediction data\n",
    "        dao.write_csv(pred_file, df_pred)\n",
    "\n",
    "    print(f\"{stock} processed for {current_date}\")\n",
    "\n",
    "# Main function to execute multi-threading\n",
    "def process_stocks_in_parallel(stocks, data_dump_path, run_type, current_date):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        executor.map(lambda stock: process_stock(\n",
    "            stock, data_dump_path, run_type, current_date), stocks)\n",
    "\n",
    "\n",
    "# Call the main function to process the stocks in parallel\n",
    "process_stocks_in_parallel(stocks, data_dump_path, run_type, current_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backtest the predictions and create the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL\n",
      "NVDA\n",
      "TSLA\n",
      "AMZN\n",
      "AMD\n",
      "NFLX\n",
      "MSFT\n",
      "GOOGL\n",
      "DIS\n",
      "META\n",
      "SMR\n",
      "SMMT\n",
      "WGS\n",
      "MSTR\n",
      "SMTC\n",
      "STLA\n",
      "CSAN\n",
      "SBS\n",
      "DB\n",
      "FIVE\n",
      "NVCR\n",
      "TMC\n"
     ]
    }
   ],
   "source": [
    "# GET THE SUMMARY FILES IF IT EXISTS ELSE CREATE NEW\n",
    "prediction_summary_file = summaries_files_path + run_type + \\\n",
    "    \"_prediction_summary_\"+str(current_date)+\".csv\"\n",
    "prediction_summary_df = utils.get_summary_file(name=prediction_summary_file)\n",
    "\n",
    "# LOOP OVER STOCKS AND PROCESS EACH STOCK\n",
    "for stock in stocks:\n",
    "    print(stock)\n",
    "\n",
    "    # DECLARE THE FILES\n",
    "    pred_file = data_dump_path + run_type + \"_\" + \\\n",
    "        stock + \"_df_pred_\" + str(current_date) + \".csv\"\n",
    "    tested_file = data_dump_path + run_type + \"_\" + \\\n",
    "        stock + \"_df_tested_\" + str(current_date) + \".csv\"\n",
    "\n",
    "    # READ LOAD THE PRED FILE\n",
    "    file_to_test_df = pd.read_csv(pred_file)\n",
    "\n",
    "    # BACKTEST THE SIGNAL\n",
    "    df_tested, cumulative_return, sharpe_ratio, sortino_ratio, max_drawdown, calmar_ratio = backtesting.backtest(\n",
    "        file_to_test_df)\n",
    "\n",
    "    # WRITE THE PREDICTION BACKTESTED DATA\n",
    "    dao.write_csv(tested_file, df_tested)\n",
    "\n",
    "    # GET THE SUMMARY\n",
    "    start_iloc = 1\n",
    "    end_iloc = 30\n",
    "    signals = df_tested[\"Signal_Text\"].iloc[start_iloc:end_iloc +\n",
    "                                            1].reset_index(drop=True)\n",
    "    signal_text, signal_number = prediction.get_weighted_signal(signals)\n",
    "    prediction_summary_df = pd.concat([prediction_summary_df,\n",
    "                                       pd.DataFrame([[\n",
    "                                           current_date,\n",
    "                                           stock,\n",
    "                                           signal_number,\n",
    "                                           signal_text,\n",
    "                                           cumulative_return,\n",
    "                                           sharpe_ratio,\n",
    "                                           sortino_ratio,\n",
    "                                           max_drawdown,\n",
    "                                           calmar_ratio\n",
    "                                       ]],\n",
    "                                           columns=prediction_summary_df.columns)],\n",
    "                                      ignore_index=True)\n",
    "    prediction_summary_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # WRITE THE SUMMARY DATA\n",
    "    dao.write_csv(prediction_summary_file, prediction_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate exploratory trades from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exploratory_exec_flag set to: False\n"
     ]
    }
   ],
   "source": [
    "if exploratory_exec_flag:\n",
    "    for stock in stocks:\n",
    "        print(stock)\n",
    "        # DECLARE THE FILES\n",
    "        pred_file = data_dump_path + run_type + \"_\" + \\\n",
    "            stock + \"_df_pred_\" + str(current_date) + \".csv\"\n",
    "        exploratory_simulation_file = data_dump_path + run_type + \"_\" + \\\n",
    "            stock + \"_exploratory_simulation_\" + str(current_date) + \".json\"\n",
    "\n",
    "        # PREPARE THE SIM DATA\n",
    "        file_to_sim_df = pd.read_csv(pred_file)\n",
    "        data_to_sim_df = file_to_sim_df[[\n",
    "            'High', 'Low', 'Close', 'Date', 'TICKER', 'Signal_Text']]\n",
    "\n",
    "        # INITIALISE THE SIM VALUES\n",
    "        start_iloc = 1  # 0 is the latest day\n",
    "        end_iloc = 31  # 29 is the 30th day\n",
    "\n",
    "        # DO THE SIM\n",
    "        simulation_df = simulation.simulate_exploratory_trading(\n",
    "            data_to_sim_df, start_iloc, end_iloc, initial_funds)\n",
    "\n",
    "        # WRITE THE DATA\n",
    "        dao.write_json(exploratory_simulation_file, simulation_df)\n",
    "else:\n",
    "    print('exploratory_exec_flag set to: '+str(exploratory_exec_flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarise the exploratory simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exploratory_exec_flag set to: False\n"
     ]
    }
   ],
   "source": [
    "if exploratory_exec_flag:\n",
    "    # GET THE SUMMARY FILES IF IT EXISTS ELSE CREATE NEW\n",
    "    exploratory_simulation_summary_file = summaries_files_path + run_type + \\\n",
    "        \"_exploratory_simulation_summary_\"+str(current_date)+\".csv\"\n",
    "    exploratory_simulation_summary_df = utils.get_simulation_file(\n",
    "        exploratory_simulation_summary_file)\n",
    "\n",
    "    # LOOP OVER STOCKS AND PROCESS EACH STOCK\n",
    "    for stock in stocks:\n",
    "        print(stock)\n",
    "\n",
    "        # DECLARE THE FILES\n",
    "        exploratory_simulation_file = data_dump_path + run_type + \"_\" + \\\n",
    "            stock + \"_exploratory_simulation_\" + str(current_date) + \".json\"\n",
    "\n",
    "        # READ FILE\n",
    "        file_to_json = dao.read_json(exploratory_simulation_file)\n",
    "\n",
    "        positions = []\n",
    "        for json in file_to_json['transactions']:\n",
    "            positions.append(json['position'])\n",
    "        series = pd.Series(positions)\n",
    "        days_to_consider = len(positions)\n",
    "\n",
    "        signal_text, signal_number = prediction.get_weighted_signal(series)\n",
    "\n",
    "        exploratory_simulation_summary_df = pd.concat([exploratory_simulation_summary_df,\n",
    "                                                       pd.DataFrame([[\n",
    "                                                           current_date,\n",
    "                                                           stock,\n",
    "                                                           file_to_json['closing_stock_price'],\n",
    "                                                           file_to_json['final_cash_balance'],\n",
    "                                                           file_to_json['unrealized_gains_losses'],\n",
    "                                                           file_to_json['unrealized_gain_loss_%'],\n",
    "                                                           file_to_json['units_held'],\n",
    "                                                           file_to_json['average_price_per_unit'],\n",
    "                                                           signal_text,\n",
    "                                                           signal_number\n",
    "                                                       ]],\n",
    "                                                           columns=exploratory_simulation_summary_df.columns)],\n",
    "                                                      ignore_index=True)\n",
    "        exploratory_simulation_summary_df.drop_duplicates(inplace=True)\n",
    "\n",
    "        dao.write_csv(exploratory_simulation_summary_file,\n",
    "                      exploratory_simulation_summary_df)\n",
    "else:\n",
    "    print('exploratory_exec_flag set to: '+str(exploratory_exec_flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate current portfolio trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ticker': 'AAPL', 'date': '2024-12-02', 'units': 0.044192, 'average_buying_price': 52.72}\n",
      "{'ticker': 'NVDA', 'date': '2024-12-04', 'units': 0.0415, 'average_buying_price': 198.55}\n",
      "{'ticker': 'TSLA', 'date': '2024-11-19', 'units': 0.002977, 'average_buying_price': 335.91}\n",
      "{'ticker': 'AMZN', 'date': '2024-11-19', 'units': 0.005041, 'average_buying_price': 198.37}\n",
      "{'ticker': 'AMD', 'date': '2024-12-04', 'units': 0.093372, 'average_buying_price': 137.51}\n",
      "{'ticker': 'NFLX', 'date': '2024-11-19', 'units': 0.00119, 'average_buying_price': 840.34}\n",
      "{'ticker': 'MSFT', 'date': '2024-11-19', 'units': 0.002416, 'average_buying_price': 413.91}\n",
      "{'ticker': 'GOOGL', 'date': '2024-11-19', 'units': 0.005736, 'average_buying_price': 174.34}\n",
      "{'ticker': 'DIS', 'date': '2024-11-19', 'units': 0.008897, 'average_buying_price': 112.4}\n",
      "{'ticker': 'META', 'date': '2024-11-19', 'units': 0.001815, 'average_buying_price': 550.96}\n",
      "{'ticker': 'SMR', 'date': '2024-11-19', 'units': 0.0741, 'average_buying_price': 26.99}\n",
      "{'ticker': 'SMMT', 'date': '2024-11-19', 'units': 0.087167, 'average_buying_price': 18.7}\n",
      "{'ticker': 'WGS', 'date': '2024-12-02', 'units': 0.06855, 'average_buying_price': 78.05}\n",
      "{'ticker': 'MSTR', 'date': '2024-12-02', 'units': 0.006117, 'average_buying_price': 393.98}\n",
      "{'ticker': 'SMTC', 'date': '2024-12-02', 'units': 0.034824, 'average_buying_price': 64.61}\n",
      "{'ticker': 'STLA', 'date': '2024-12-02', 'units': 0.175725, 'average_buying_price': 12.24}\n",
      "{'ticker': 'CSAN', 'date': '2024-12-04', 'units': 0.761575, 'average_buying_price': 6.57}\n",
      "{'ticker': 'SBS', 'date': '2024-12-02', 'units': 0.216748, 'average_buying_price': 15.23}\n",
      "{'ticker': 'DB', 'date': '2024-12-02', 'units': 0.175695, 'average_buying_price': 17.08}\n",
      "{'ticker': 'FIVE', 'date': '2024-12-02', 'units': 0.02819, 'average_buying_price': 96.84}\n",
      "{'ticker': 'NVCR', 'date': '2024-12-02', 'units': 0.043103, 'average_buying_price': 29.0}\n",
      "{'ticker': 'TMC', 'date': '2024-12-04', 'units': 5.555555, 'average_buying_price': 0.9}\n"
     ]
    }
   ],
   "source": [
    "if portfolio_exec_flag:\n",
    "    portfolio_file = static_files_path + portfolio_json_file\n",
    "    portfolio_stocks = dao.read_json(portfolio_file)\n",
    "\n",
    "    for stock in portfolio_stocks:\n",
    "        print(stock)\n",
    "\n",
    "        # DECLARE THE FILES\n",
    "        pred_file = data_dump_path + run_type + \"_\" + \\\n",
    "            stock['ticker'] + \"_df_pred_\" + str(current_date) + \".csv\"\n",
    "        portfolio_simulation_file = data_dump_path + run_type + \"_\" + \\\n",
    "            stock['ticker'] + \"_portfolio_simulation_\" + \\\n",
    "            str(current_date) + \".json\"\n",
    "\n",
    "        # PREPARE THE SIM DATA\n",
    "        file_to_sim_df = pd.read_csv(pred_file)\n",
    "        data_to_sim_df = file_to_sim_df[[\n",
    "            'High', 'Low', 'Close', 'Date', 'TICKER', 'Signal_Text']]\n",
    "\n",
    "        # GET ILOCS\n",
    "        start_iloc = 1\n",
    "        end_iloc = data_to_sim_df.index[data_to_sim_df['Date'] == stock['date']].tolist()[\n",
    "            0]\n",
    "        units_held = stock['units']\n",
    "        avg_price = stock['average_buying_price']\n",
    "\n",
    "        # DO THE SIM\n",
    "        simulation_df = simulation.simulate_portfolio_trades(\n",
    "            data_to_sim_df, start_iloc, end_iloc, initial_funds, units_held, avg_price)\n",
    "\n",
    "        # WRITE THE DATA\n",
    "        dao.write_json(portfolio_simulation_file, simulation_df)\n",
    "else:\n",
    "    print('portfolio_exec_flag set to: '+str(portfolio_exec_flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarise the potrfolio simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL\n",
      "NVDA\n",
      "TSLA\n",
      "AMZN\n",
      "AMD\n",
      "NFLX\n",
      "MSFT\n",
      "GOOGL\n",
      "DIS\n",
      "META\n",
      "SMR\n",
      "SMMT\n",
      "WGS\n",
      "MSTR\n",
      "SMTC\n",
      "STLA\n",
      "CSAN\n",
      "SBS\n",
      "DB\n",
      "FIVE\n",
      "NVCR\n",
      "TMC\n"
     ]
    }
   ],
   "source": [
    "if portfolio_exec_flag:\n",
    "    # GET THE SUMMARY FILES IF IT EXISTS ELSE CREATE NEW\n",
    "    portfolio_simulation_summary_file = summaries_files_path + run_type + \\\n",
    "        \"_portfolio_simulation_summary_\"+str(current_date)+\".csv\"\n",
    "    portfolio_simulation_summary_df = utils.get_simulation_file(\n",
    "        portfolio_simulation_summary_file)\n",
    "\n",
    "    # LOOP OVER STOCKS AND PROCESS EACH STOCK\n",
    "    for stock in stocks:\n",
    "        print(stock)\n",
    "\n",
    "        # DECLARE THE FILES\n",
    "        portfolio_simulation_file = data_dump_path + run_type + \"_\" + \\\n",
    "            stock + \"_portfolio_simulation_\" + str(current_date) + \".json\"\n",
    "\n",
    "        # READ FILE\n",
    "        file_to_json = dao.read_json(portfolio_simulation_file)\n",
    "\n",
    "        positions = []\n",
    "        for json in file_to_json['transactions']:\n",
    "            positions.append(json['position'])\n",
    "        series = pd.Series(positions)\n",
    "        days_to_consider = len(positions)\n",
    "\n",
    "        signal_text, signal_number = prediction.get_weighted_signal(series)\n",
    "\n",
    "        portfolio_simulation_summary_df = pd.concat([portfolio_simulation_summary_df,\n",
    "                                                     pd.DataFrame([[\n",
    "                                                         current_date,\n",
    "                                                         stock,\n",
    "                                                         file_to_json['closing_stock_price'],\n",
    "                                                         file_to_json['final_cash_balance'],\n",
    "                                                         file_to_json['unrealized_gains_losses'],\n",
    "                                                         file_to_json['unrealized_gain_loss_%'],\n",
    "                                                         file_to_json['units_held'],\n",
    "                                                         file_to_json['average_price_per_unit'],\n",
    "                                                         signal_text,\n",
    "                                                         signal_number\n",
    "                                                     ]],\n",
    "                                                         columns=portfolio_simulation_summary_df.columns)],\n",
    "                                                    ignore_index=True)\n",
    "        portfolio_simulation_summary_df.drop_duplicates(inplace=True)\n",
    "\n",
    "        dao.write_csv(portfolio_simulation_summary_file,\n",
    "                      portfolio_simulation_summary_df)\n",
    "else:\n",
    "    print('portfolio_exec_flag set to: '+str(portfolio_exec_flag))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
